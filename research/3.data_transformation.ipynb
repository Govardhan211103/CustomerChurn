{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\MEDHA TRUST\\\\Documents\\\\Govardhan\\\\ML\\\\github\\\\CustomerChurn\\\\CustomerChurn\\\\research'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\MEDHA TRUST\\\\Documents\\\\Govardhan\\\\ML\\\\github\\\\CustomerChurn\\\\CustomerChurn'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## command should be in CustomerChurn directory\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen = True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ChurnPrediction.constants import *\n",
    "from ChurnPrediction.utils.common import read_yaml, create_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH,\n",
    "            schema_filepath = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directory([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directory([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ChurnPrediction import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_transformer_object(self) -> ColumnTransformer:\n",
    "        try:\n",
    "            # Feature lists\n",
    "            cat_columns = [\n",
    "                'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',\n",
    "                'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "                'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
    "            ]\n",
    "            num_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "            # Pipelines for features\n",
    "            numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('label', OrdinalEncoder())\n",
    "            ])\n",
    "\n",
    "            preprocessor = ColumnTransformer([\n",
    "                ('numerical_pipeline', numeric_transformer, num_columns),\n",
    "                ('categorical_pipeline', categorical_transformer, cat_columns)\n",
    "            ])\n",
    "\n",
    "            return preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def transform_data(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "\n",
    "        # Remove 'customerID' as it is unique for each customer\n",
    "        data.drop(['customerID'], axis=1, inplace=True)\n",
    "\n",
    "        # Change the datatypes of 'SeniorCitizen' and 'TotalCharges' features\n",
    "        data['SeniorCitizen'] = data['SeniorCitizen'].astype('object')\n",
    "        data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "\n",
    "        # Encode 'Churn' (the target feature) into numeric values\n",
    "        data['Churn'] = data['Churn'].replace({'Yes': 1, 'No': 0})\n",
    "        data['Churn'] = data['Churn'].astype('object')\n",
    "\n",
    "        # Split features and target\n",
    "        data_X = data.drop(['Churn'], axis=1)\n",
    "        data_y = data['Churn']\n",
    "\n",
    "        # Get the preprocessor object\n",
    "        preprocessor_object = self.get_transformer_object()\n",
    "        \n",
    "        # Preprocess the features\n",
    "        data_X_preprocessed = preprocessor_object.fit_transform(pd.DataFrame(data_X))\n",
    "\n",
    "        # Create a DataFrame with the preprocessed features\n",
    "        transformed_feature_names = preprocessor_object.get_feature_names_out(input_features = data_X.columns)\n",
    "        data_X_scaled = pd.DataFrame(data_X_preprocessed, columns=transformed_feature_names)\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        train_data, test_data = train_test_split(pd.concat([data_X_scaled, data_y], axis = 1), test_size=0.2, random_state=42, stratify = data_y)\n",
    "\n",
    "\n",
    "        # Save the train and test data to CSV files\n",
    "        train_data.to_csv(self.config.train_data_path, index=False)\n",
    "        test_data.to_csv(self.config.test_data_path, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-15 15:37:59,092: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-10-15 15:37:59,095: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-10-15 15:37:59,097: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-15 15:37:59,097: INFO: common: directory: artifacts created successfully]\n",
      "[2023-10-15 15:37:59,097: INFO: common: directory: artifacts/data_transformation created successfully]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config = data_transformation_config)\n",
    "    data_transformation.transform_data()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
